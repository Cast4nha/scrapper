#!/usr/bin/env python3
"""
Analisa a resposta do ScrapingDog
"""

import os
import glob
from bs4 import BeautifulSoup

def analyze_scrapingdog_response():
    """Analisa a resposta do ScrapingDog"""
    print("üîç ANALISANDO RESPOSTA DO SCRAPINGDOG")
    print("=" * 50)
    
    # Encontrar arquivo de resposta
    response_files = glob.glob("scrapingdog_response_*.html")
    
    if not response_files:
        print("‚ùå Nenhum arquivo de resposta encontrado")
        return
    
    response_file = response_files[0]
    print(f"üìÑ Analisando: {response_file}")
    
    try:
        # Ler arquivo
        with open(response_file, 'r', encoding='utf-8', errors='ignore') as f:
            html_content = f.read()
        
        print(f"üìè Tamanho do HTML: {len(html_content)} caracteres")
        
        # Parsear HTML
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Verificar t√≠tulo
        title = soup.title.string if soup.title else "Sem t√≠tulo"
        print(f"üì∞ T√≠tulo: {title}")
        
        # Verificar URL atual
        current_url = soup.find('meta', {'property': 'og:url'})
        if current_url:
            print(f"üåê URL: {current_url.get('content')}")
        
        # Procurar por elementos de bilhete
        print("\nüîç PROCURANDO ELEMENTOS DO BILHETE:")
        print("-" * 40)
        
        # Procurar por containers de jogos
        game_containers = soup.find_all(['div', 'section'], class_=lambda x: x and any(word in x.lower() for word in ['game', 'bet', 'item', 'ticket']))
        print(f"üéÆ Containers de jogos encontrados: {len(game_containers)}")
        
        # Procurar por texto que contenha padr√µes de jogos
        text_content = soup.get_text()
        
        # Padr√µes para encontrar jogos
        import re
        
        patterns = [
            r'([A-Za-z√Ä-√ø\s]+):\s*([A-Za-z√Ä-√ø\s]+)\s+(\d{2}/\d{2}\s+\d{2}:\d{2})',
            r'([A-Za-z√Ä-√ø\s]+)\s+x\s+([A-Za-z√Ä-√ø\s]+)',
            r'Vencedor:\s*([A-Za-z√Ä-√ø\s]+)',
            r'(\d+\.\d+)',  # Odds
            r'Total odds\s*(\d+[,\d]*)',
            r'Poss√≠vel pr√™mio\s*(R\$\s*\d+[,\d]*)'
        ]
        
        print("\nüîç PADR√ïES ENCONTRADOS:")
        print("-" * 30)
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, text_content, re.MULTILINE)
            if matches:
                print(f"Padr√£o {i+1}: {len(matches)} matches")
                for match in matches[:3]:  # Mostrar apenas os primeiros 3
                    print(f"   - {match}")
                if len(matches) > 3:
                    print(f"   ... e mais {len(matches) - 3}")
            else:
                print(f"Padr√£o {i+1}: Nenhum match")
        
        # Procurar por inputs
        inputs = soup.find_all('input')
        print(f"\nüìù Inputs encontrados: {len(inputs)}")
        
        for i, input_elem in enumerate(inputs):
            input_type = input_elem.get('type', '')
            input_placeholder = input_elem.get('placeholder', '')
            input_value = input_elem.get('value', '')
            
            if input_placeholder or input_value:
                print(f"   Input {i+1}: tipo={input_type}, placeholder='{input_placeholder}', value='{input_value}'")
        
        # Procurar por bot√µes
        buttons = soup.find_all('button')
        print(f"\nüîò Bot√µes encontrados: {len(buttons)}")
        
        for i, button in enumerate(buttons):
            button_text = button.get_text(strip=True)
            button_class = button.get('class', [])
            if button_text:
                print(f"   Bot√£o {i+1}: '{button_text}' (classes: {button_class})")
        
        # Verificar se h√° redirecionamento ou erro
        error_indicators = ['error', 'not found', '404', 'access denied', 'forbidden']
        for indicator in error_indicators:
            if indicator in text_content.lower():
                print(f"\n‚ö†Ô∏è  Indicador de erro encontrado: '{indicator}'")
        
        # Salvar texto limpo para an√°lise
        clean_text_file = "scrapingdog_clean_text.txt"
        with open(clean_text_file, 'w', encoding='utf-8') as f:
            f.write(text_content)
        print(f"\nüìÑ Texto limpo salvo em: {clean_text_file}")
        
        print("\n‚úÖ An√°lise conclu√≠da!")
        
    except Exception as e:
        print(f"‚ùå Erro ao analisar: {str(e)}")

if __name__ == "__main__":
    analyze_scrapingdog_response()
